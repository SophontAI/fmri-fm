model_name: "HCP_large_pt1" 
datasets_to_include: "BOTH" # ["NSD", "HCP", "BOTH"]
hcp_flat_path: "/home/connor/fmri-fm/datasets/HCP-Flat"
nsd_flat_path: "/home/connor/fmri-fm/datasets/NSD-Flat"

model_size: "large"
global_pool: False
cls_forward: False

gsr: False # we should use gsr=False but it's a lot slower due to how the tars were built with gsr=True
use_contrastive_loss: False
use_decoder_contrastive_loss: False
cls_embed: False
decoder_cls_embed: False
use_source_embeds: False
source_embed_mode: "add"  # "append" or "add"
source_embed_train_mode: "ce" # ema or ce

patch_size: 16
pct_masks_to_decode: 1
decoder_embed_dim: 512
num_frames: 16
mask_ratio: .75
pred_t_dim: 8
t_patch_size: 2
no_qkv_bias: False
sep_pos_embed: True # has to be true for source embeds
trunc_init: False
norm_pix_loss: False
contrastive_loss_weight: 1.0

train_split: .1

# Training Configs
batch_size: 32 # change from 64 to 32 if contrastive, as you'll effectively be doubling bs
same_run_samples: 16 # number of samples taken from same run (for efficiency); cannot be larger than num_frames
num_workers: 1 
num_epochs: 200 #200 for BOTH, 100 for NSD or HCP
seed: 42
base_lr: 1.0e-3 # Keep the x.0 else will be converted to string
num_samples_per_epoch: 200000 #200000
grad_clip: 1.0  # set 0 for no clip
grad_accumulation_steps: 1
plotting: True

# NSD dataloader specific
# num_sessions only gets used for main.ipynb, not main_finetuning.ipynb
num_sessions: 40 

# HCP dataloader specific
sub_min: 0 #990000 #0 # if greater than zero, only allows files where subject number is >= sub_min

# Saving progress
ckpt_saving: True
ckpt_interval: 25 # 25 # in epochs
print_interval: 20 # in steps
resume_from_ckpt: True
wandb_log: True
wandb_entity: "paulscotti" # set this to your own wanbd username (or team name)!
wandb_rand: 0 # 0 means set to random; if resuming a wandb run check what its wandb_rand was and change it here
