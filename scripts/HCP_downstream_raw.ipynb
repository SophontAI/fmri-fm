{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 634571\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from mae_utils.flat_models import *\n",
    "import h5py\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# following fixes a Conv3D CUDNN_NOT_SUPPORTED error\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ## MODEL TO LOAD ##\n",
    "# model_name = \"HCPflat_large_gsrFalse_\"\n",
    "# parquet_folder = \"epoch99\"\n",
    "\n",
    "# # outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "# outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "\n",
    "# print(\"outdir\", outdir)\n",
    "# # Load previous config.yaml if available\n",
    "# if os.path.exists(f\"{outdir}/config.yaml\"):\n",
    "#     config = yaml.load(open(f\"{outdir}/config.yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "#     print(f\"Loaded config.yaml from ckpt folder {outdir}\")\n",
    "#     # create global variables from the config\n",
    "#     print(\"\\n__CONFIG__\")\n",
    "#     for attribute_name in config.keys():\n",
    "#         print(f\"{attribute_name} = {config[attribute_name]}\")\n",
    "#         globals()[attribute_name] = config[f'{attribute_name}']\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# world_size = os.getenv('WORLD_SIZE')\n",
    "# if world_size is None: \n",
    "#     world_size = 1\n",
    "# else:\n",
    "#     world_size = int(world_size)\n",
    "# print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "# if utils.is_interactive():\n",
    "#     # Following allows you to change functions in models.py or utils.py and \n",
    "#     # have this notebook automatically update with your revisions\n",
    "#     %load_ext autoreload\n",
    "#     %autoreload 2\n",
    "\n",
    "# batch_size = probe_batch_size\n",
    "# num_epochs = probe_num_epochs\n",
    "\n",
    "# data_type = torch.float32 # change depending on your mixed_precision\n",
    "# global_batch_size = batch_size * world_size\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "hcp_flat_path = \"/weka/proj-medarc/shared/HCP-Flat\"\n",
    "seed = 42\n",
    "num_frames = 16\n",
    "gsr = False\n",
    "num_workers = 10\n",
    "batch_size = 128\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "utils.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc00235a-a9e1-4dc4-8fd3-359c1d91bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UNCOMMENT THIS TO SAVE THE HCP-FLAT IN HDF5 FORMAT\n",
    "\n",
    "\n",
    "# from torch.utils.data import default_collate\n",
    "# from mae_utils.flat import load_hcp_flat_mask\n",
    "# from mae_utils.flat import create_hcp_flat\n",
    "# from mae_utils.flat import batch_unmask\n",
    "# import mae_utils.visualize as vis\n",
    "\n",
    "\n",
    "# batch_size = 26\n",
    "# print(f\"changed batch_size to {batch_size}\")\n",
    "\n",
    "# ## Test ##\n",
    "# datasets_to_include = \"HCP\"\n",
    "# assert \"HCP\" in datasets_to_include\n",
    "# test_dataset = create_hcp_flat(root=hcp_flat_path, \n",
    "#                 clip_mode=\"event\", frames=num_frames, shuffle=False, gsr=gsr, sub_list = 'test')\n",
    "# test_dl = wds.WebLoader(\n",
    "#     test_dataset.batched(batch_size, partial=False, collation_fn=default_collate),\n",
    "#     batch_size=None,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# ## Train ##\n",
    "# assert \"HCP\" in datasets_to_include\n",
    "# train_dataset = create_hcp_flat(root=hcp_flat_path, \n",
    "#                 clip_mode=\"event\", frames=num_frames, shuffle=False, gsr=gsr, sub_list = 'train')\n",
    "# train_dl = wds.WebLoader(\n",
    "#     train_dataset.batched(batch_size, partial=False, collation_fn=default_collate),\n",
    "#     batch_size=None,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# def flatten_meta(meta_dict):\n",
    "#     \"\"\"\n",
    "#     Flatten the meta dictionary by:\n",
    "#     - Replacing single-item lists with the item itself.\n",
    "#     - Converting tensors to scalar numbers.\n",
    "#     \"\"\"\n",
    "#     flattened = {}\n",
    "#     for key, value in meta_dict.items():\n",
    "#         if isinstance(value, list):\n",
    "#             if len(value) == 1:\n",
    "#                 flattened[key] = value[0]  # Replace list with its single item\n",
    "#             else:\n",
    "#                 flattened[key] = value  # Keep as is if multiple items\n",
    "#         elif isinstance(value, torch.Tensor):\n",
    "#             # Convert tensor to scalar\n",
    "#             if value.numel() == 1:\n",
    "#                 flattened[key] = value.item()\n",
    "#             else:\n",
    "#                 flattened[key] = value.tolist()  # Convert multi-element tensor to list\n",
    "#         else:\n",
    "#             flattened[key] = value  # Keep the value as is\n",
    "#     return flattened\n",
    "\n",
    "# import h5py\n",
    "# meta_array = np.array([], dtype=object)\n",
    "# # Open an HDF5 file in write mode\n",
    "# with h5py.File('train_hcp.hdf5', 'w') as h5f:\n",
    "#     flatmaps_dset = None\n",
    "    \n",
    "#     total_samples = 0\n",
    "\n",
    "#     for i, batch in tqdm(enumerate(train_dl), total = 120000):\n",
    "#         images = batch['image'][0]\n",
    "#         meta = batch['meta']\n",
    "#         batch_size = images.shape[0]\n",
    "#         meta_serializable = meta.copy()\n",
    "        \n",
    "        \n",
    "#         # Step 2: Serialize the dictionary to a JSON string\n",
    "#         meta_str = json.dumps(flatten_meta(meta_serializable), indent=4)\n",
    "#         meta_array = np.append(meta_array, meta_str)\n",
    "#         if flatmaps_dset is None:\n",
    "#             # Initialize datasets with unlimited (None) maxshape along the first axis\n",
    "#             flatmaps_shape = (0,) + images.shape[1:]\n",
    "#             flatmaps_maxshape = (None,) + images.shape[1:]\n",
    "\n",
    "#             flatmaps_dset = h5f.create_dataset(\n",
    "#                 'flatmaps',\n",
    "#                 shape=flatmaps_shape,\n",
    "#                 maxshape=flatmaps_maxshape,\n",
    "#                 dtype=np.float16,\n",
    "#                 chunks=True  # Enable chunking for efficient resizing\n",
    "#             )\n",
    "\n",
    "#         # Resize datasets to accommodate new data\n",
    "#         flatmaps_dset.resize(total_samples + batch_size, axis=0)\n",
    "\n",
    "#         # Write data to the datasets\n",
    "#         flatmaps_dset[total_samples:total_samples + batch_size] = images.numpy().astype(np.float16)\n",
    "\n",
    "#         total_samples += batch_size\n",
    "        \n",
    "#     print(f\"Processed {total_samples} samples\")\n",
    "# np.save('metadata_test_HCP.npy', meta_array)\n",
    "\n",
    "\n",
    "# import h5py\n",
    "# meta_array = np.array([], dtype=object)\n",
    "# # Open an HDF5 file in write mode\n",
    "# with h5py.File('test_hcp.hdf5', 'w') as h5f:\n",
    "#     flatmaps_dset = None\n",
    "    \n",
    "#     total_samples = 0\n",
    "\n",
    "#     for i, batch in tqdm(enumerate(test_dl), total = 12000):\n",
    "#         images = batch['image'][0]\n",
    "#         meta = batch['meta']\n",
    "#         batch_size = images.shape[0]\n",
    "#         meta_serializable = meta.copy()\n",
    "        \n",
    "        \n",
    "#         # Step 2: Serialize the dictionary to a JSON string\n",
    "#         meta_str = json.dumps(flatten_meta(meta_serializable), indent=4)\n",
    "#         meta_array = np.append(meta_array, meta_str)\n",
    "#         if flatmaps_dset is None:\n",
    "#             # Initialize datasets with unlimited (None) maxshape along the first axis\n",
    "#             flatmaps_shape = (0,) + images.shape[1:]\n",
    "#             flatmaps_maxshape = (None,) + images.shape[1:]\n",
    "\n",
    "#             flatmaps_dset = h5f.create_dataset(\n",
    "#                 'flatmaps',\n",
    "#                 shape=flatmaps_shape,\n",
    "#                 maxshape=flatmaps_maxshape,\n",
    "#                 dtype=np.float16,\n",
    "#                 chunks=True  # Enable chunking for efficient resizing\n",
    "#             )\n",
    "\n",
    "#         # Resize datasets to accommodate new data\n",
    "#         flatmaps_dset.resize(total_samples + batch_size, axis=0)\n",
    "\n",
    "#         # Write data to the datasets\n",
    "#         flatmaps_dset[total_samples:total_samples + batch_size] = images.numpy().astype(np.float16)\n",
    "\n",
    "#         total_samples += batch_size\n",
    "        \n",
    "#     print(f\"Processed {total_samples} samples\")\n",
    "# np.save('metadata_train_HCP.npy', meta_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98629867-6d64-45ef-823d-5f4fdb279ec7",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e66e2c0-6a73-42d7-8414-3b8acafdb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = h5py.File('/weka/proj-fmri/ckadirt/fMRI-foundation-model/src/train_hcp.hdf5', 'r')\n",
    "flatmaps_train = f_train['flatmaps']\n",
    "\n",
    "f_test = h5py.File('/weka/proj-fmri/ckadirt/fMRI-foundation-model/src/test_hcp.hdf5', 'r')\n",
    "flatmaps_test = f_test['flatmaps']\n",
    "\n",
    "metadata_train = np.load('/weka/proj-fmri/ckadirt/fMRI-foundation-model/src/metadata_train_HCP.npy', allow_pickle=True)\n",
    "metadata_test = np.load('/weka/proj-fmri/ckadirt/fMRI-foundation-model/src/metadata_test_HCP.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9788d371-c6ce-44d9-8ebc-d0a13d202378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import json\n",
    "# import os\n",
    "# import pickle\n",
    "# from pathlib import Path\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# target = \"trial_type\"\n",
    "# print(f\"Target: {target}\")\n",
    "\n",
    "# # train_features = pd.read_parquet(f\"{outdir}/{parquet_folder}/HCP/train.parquet\")\n",
    "# # test_features = pd.read_parquet(f\"{outdir}/{parquet_folder}/HCP_/test.parquet\")\n",
    "\n",
    "# # print(f\"train: {train_features.shape}, test: {test_features.shape}\")\n",
    "# # print(f\"test: {test_features.shape}\")\n",
    "\n",
    "# X_train = np.array(flatmaps_train[0:5000])\n",
    "# # flatten the flatmaps\n",
    "# X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "# X_test = np.array(flatmaps_test[0:1000])\n",
    "# X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "# print(f\"X_test: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# # if target == \"task\":\n",
    "# #     labels_train = train_features[\"task\"].str.rstrip(\"1234\").values\n",
    "# #     labels_test = test_features[\"task\"].str.rstrip(\"1234\").values\n",
    "# # elif target == \"trial_type\":\n",
    "# #     labels_train = train_features[\"trial_type\"].values\n",
    "# #     labels_test = test_features[\"trial_type\"].values\n",
    "\n",
    "# labels_train = [json.loads(string)['trial_type'] for string in metadata_train[0:5000]]\n",
    "# labels_test = [json.loads(string)['trial_type'] for string in metadata_test[0:1000]]\n",
    "\n",
    "# label_enc = LabelEncoder()\n",
    "# y_train = label_enc.fit_transform(labels_train)\n",
    "# y_test = label_enc.transform(labels_test)\n",
    "\n",
    "# print(f\"classes ({len(label_enc.classes_)}): {label_enc.classes_}\")\n",
    "# print(\n",
    "#     f\"\\ny_train: {y_train.shape} {y_train[:20]}\\n\"\n",
    "#     f\"y_test: {y_test.shape} {y_test[:20]}\"\n",
    "# )\n",
    "# # del train_features, test_features\n",
    "\n",
    "# train_ind, val_ind = train_test_split(\n",
    "#     np.arange(len(X_train)), train_size=0.9, random_state=42\n",
    "# )\n",
    "# print(\n",
    "#     f\"\\ntrain_ind: {len(train_ind)} {train_ind[:10]}\\n\"\n",
    "#     f\"val_ind: {len(val_ind)} {val_ind[:10]}\"\n",
    "# )\n",
    "# X_train, X_val = X_train[train_ind], X_train[val_ind]\n",
    "# y_train, y_val = y_train[train_ind], y_train[val_ind]\n",
    "\n",
    "# print(\"Fitting PCA projection\")\n",
    "# pca = PCA(n_components=384, whiten=True, svd_solver=\"randomized\")\n",
    "# pca.fit(X_train)\n",
    "\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_val = pca.transform(X_val)\n",
    "# X_test = pca.transform(X_test)\n",
    "\n",
    "# print(\"Fitting logistic regression\")\n",
    "# clf = LogisticRegressionCV()\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# train_acc = clf.score(X_train, y_train)\n",
    "# val_acc = clf.score(X_val, y_val)\n",
    "# test_acc = clf.score(X_test, y_test)\n",
    "\n",
    "# result = {\n",
    "#     \"target\": target,\n",
    "#     \"train_acc\": train_acc,\n",
    "#     \"val_acc\": val_acc,\n",
    "#     \"test_acc\": test_acc,\n",
    "# }\n",
    "# print(f\"Done:\\n{json.dumps(result)}\")\n",
    "# with open(f\"{outdir}/{parquet_folder}/HCP/downstream.json\", 'w') as out_json:\n",
    "#     json.dump(result, out_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52b668f-c143-40fe-b3d4-c21c7a406d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HCPFlatDataset(Dataset):\n",
    "    def __init__(self, flatmaps, metadata):\n",
    "        self.flatmaps = flatmaps\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.flatmaps[idx], json.loads(self.metadata[idx])\n",
    "\n",
    "# Loading to cpu for faster training, this can take several minutes.  Remove this [:] if you want to move one at the time.\n",
    "train_dataset = HCPFlatDataset(flatmaps_train[:], metadata_train)\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_dataset = HCPFlatDataset(flatmaps_test[:], metadata_test)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f04009-13f1-473d-9808-181a46d3e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "INCLUDE_CONDS = {\n",
    "    \"fear\",\n",
    "    \"neut\",\n",
    "    \"math\",\n",
    "    \"story\",\n",
    "    \"lf\",\n",
    "    \"lh\",\n",
    "    \"rf\",\n",
    "    \"rh\",\n",
    "    \"t\",\n",
    "    \"match\",\n",
    "    \"relation\",\n",
    "    \"mental\",\n",
    "    \"rnd\",\n",
    "    \"0bk_body\",\n",
    "    \"2bk_body\",\n",
    "    \"0bk_faces\",\n",
    "    \"2bk_faces\",\n",
    "    \"0bk_places\",\n",
    "    \"2bk_places\",\n",
    "    \"0bk_tools\",\n",
    "    \"2bk_tools\",\n",
    "}\n",
    "\n",
    "# test_data = []\n",
    "\n",
    "# # Iterate over the DataLoader with a progress bar\n",
    "# for sample in tqdm(train_dl, desc=\"Processing samples\"):\n",
    "#     x = sample['image']\n",
    "#     y = sample['meta']['trial_type']\n",
    "#     key = sample['meta']['key']\n",
    "#     print(x.shape, y, key)\n",
    "#     break\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(sorted(INCLUDE_CONDS))  # Ensure consistent ordering\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2c6527-fa75-42ba-be43-3d753ed8100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 737280\n"
     ]
    }
   ],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input except for the batch dimension\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.linear(x)\n",
    "        return out  # Raw logits\n",
    "\n",
    "# Determine the input dimension from a single sample\n",
    "# Assuming images are of shape [1, 16, 144, 320]\n",
    "sample_batch = next(iter(train_dl))\n",
    "sample_image = sample_batch[0][0]  # Shape: [1, 16, 144, 320]\n",
    "input_dim = sample_image.view(-1).size(0)\n",
    "print(f\"Input dimension: {input_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8983a0b-c810-4424-8c31-32b9921ebd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LinearClassifier(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with L2 regularization (weight_decay)\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5  # Adjust based on your needs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "num_epochs = 20  # Adjust as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ce8f26-a1b9-4e29-badc-7b9b30d0d3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in interactive notebook. Disabling W&B and ckpt saving.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "if utils.is_interactive():\n",
    "    print(\"Running in interactive notebook. Disabling W&B and ckpt saving.\")\n",
    "    wandb_log = False\n",
    "    save_ckpt = False\n",
    "\n",
    "if wandb_log:\n",
    "    wandb_project = 'fMRI-foundation-model'\n",
    "    wandb_config = {\n",
    "        \"model_name\": \"HCPflat_raw\",\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\", wandb_config)\n",
    "    random_id = random.randint(0, 100000)\n",
    "    print(\"wandb_id:\", \"HCPflat_raw\" + f\"_{random_id}\")\n",
    "    wandb.init(\n",
    "        id=\"HCPflat_raw\" + f\"_{random_id}\",\n",
    "        project=wandb_project,\n",
    "        name=\"HCPflat_raw\",\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43535f4-0f6e-4655-b4f0-926416685358",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    step = 0\n",
    "\n",
    "    # with torch.amp.autocast(device_type='cuda'):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    for  batch in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        images = batch[0].to(device).float()  # Shape: [batch_size, 1, 16, 144, 320]\n",
    "        labels = batch[1]['trial_type']  # List of labels\n",
    "        \n",
    "        encoded_labels = label_encoder.transform(labels)\n",
    "        encoded_labels = torch.tensor(encoded_labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)  # Shape: [num_train_samples, num_classes]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, encoded_labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        correct_train += (predicted == encoded_labels).sum().item()\n",
    "        total_train += encoded_labels.size(0)\n",
    "        \n",
    "        step = step + 1\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step [{step}/{len(train_dl)}] - Training Loss: {loss.item():.4f} - Training Accuracy: {100 * correct_train / total_train:.2f}%\")\n",
    "        # thth\n",
    "\n",
    "    epoch_train_loss = running_train_loss / total_train if total_train > 0 else 0.0\n",
    "    train_accuracy = 100 * correct_train / total_train if total_train > 0 else 0.0\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            images = batch[0].to(device).float().unsqueeze(1) #fix this\n",
    "            labels = batch[1]['trial_type']\n",
    "            \n",
    "            # Encode labels to integer indices\n",
    "            encoded_labels = label_encoder.transform(labels)\n",
    "            encoded_labels = torch.tensor(encoded_labels, dtype=torch.long).to(device)\n",
    "            \n",
    "        \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, encoded_labels)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == encoded_labels).sum().item()\n",
    "            total_val += encoded_labels.size(0)\n",
    "    \n",
    "    epoch_val_loss = running_val_loss / total_val if total_val > 0 else 0.0\n",
    "    val_accuracy = 100 * correct_val / total_val if total_val > 0 else 0.0\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "        f\"- Training Loss: {epoch_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}% \"\n",
    "        f\"- Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    if wandb_log:\n",
    "        wandb.log({\n",
    "            \"epoch_train_loss\": epoch_train_loss,\n",
    "            \"epoch_val_loss\": epoch_val_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "        })\n",
    "\n",
    "if save_ckpt:\n",
    "    outdir = os.path.abspath(f'checkpoints/{model_name+str(random_id)}')\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print(\"outdir\", outdir)\n",
    "    # Save model and config\n",
    "    torch.save(model.state_dict(), f\"{outdir}/model.pth\")\n",
    "    with open(f\"{outdir}/config.yaml\", 'w') as f:\n",
    "        yaml.dump(wandb_config, f)\n",
    "    print(f\"Saved model and config to {outdir}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation_env",
   "language": "python",
   "name": "foundation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
