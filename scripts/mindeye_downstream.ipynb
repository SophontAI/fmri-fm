{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "seed = 0\n",
    "import utils\n",
    "\n",
    "if utils.is_interactive():\n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2b2ad-e1ef-4262-8263-6ae9a0766caa",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127deb2-bf23-4a2e-8f86-fa8b22183546",
   "metadata": {},
   "source": [
    "### betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7063dcb1-5ae5-4a9d-a7b0-cb3323224840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5_path: /weka/proj-fmri/paulscotti/fMRI-foundation-model/src/checkpoints/NSDflat_large_gsrFalse__visualTrue_RAW/epoch99/test.h5\n",
      "<KeysViewHDF5 ['events', 'features', 'key', 'run', 'ses', 'start', 'sub']>\n"
     ]
    }
   ],
   "source": [
    "if utils.is_interactive():\n",
    "    # NSDflat_large_gsrFalse__gpFalse_visualTrue\n",
    "    # NSDflat_large_gsrFalse__visualTrue_RAW\n",
    "    # NSDflat_large_gsrFalse__gpTrue_visualTrue\n",
    "    hdf5_path = '/weka/proj-fmri/paulscotti/fMRI-foundation-model/src/checkpoints/NSDflat_large_gsrFalse__visualTrue_RAW/epoch99/test.h5'\n",
    "else:\n",
    "    hdf5_path = os.getenv('hdf5_path')\n",
    "    \n",
    "print(f\"hdf5_path: {hdf5_path}\")\n",
    "model_name = hdf5_path.split('/test.h5')[0].split('checkpoints')[-1].replace(\"/\",\"\")\n",
    "\n",
    "data_h5 = h5py.File(f'{hdf5_path}', 'r')\n",
    "print(data_h5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb2efd8-c74a-4bec-9713-00a896e2b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Subject: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions for sub 1: 100%|████████████| 40/40 [11:17<00:00, 16.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Complete.\n",
      "Data Shape: (30000, 71744)\n",
      "Image NSD73K Indices Length: 30000\n"
     ]
    }
   ],
   "source": [
    "# Load integer datasets\n",
    "sub = data_h5['sub'][:]        # Subject IDs\n",
    "ses = data_h5['ses'][:]        # Session IDs\n",
    "run = data_h5['run'][:]        # Run IDs\n",
    "start = data_h5['start'][:]    # Start indices\n",
    "\n",
    "# Load string datasets and decode them\n",
    "key = data_h5['key'][:]\n",
    "key = [k.decode('utf-8') if isinstance(k, bytes) else k for k in key]\n",
    "\n",
    "# Load and deserialize the 'events' dataset\n",
    "events_raw = data_h5['events'][:]\n",
    "events = [json.loads(e.decode('utf-8')) if isinstance(e, bytes) else json.loads(e) for e in events_raw]\n",
    "\n",
    "# Access the 'features' dataset without loading it into memory\n",
    "features = data_h5['features']\n",
    "\n",
    "num_TRs_per_image = 1  \n",
    "TR_delay = 3            \n",
    "data = [] \n",
    "image_NSD73K_indices = [] \n",
    "\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "for sub_val in [1]:\n",
    "    print(f\"\\nProcessing Subject: {sub_val}\")\n",
    "    \n",
    "    # Get indices where sub == sub_val\n",
    "    indices_sub = np.where(sub == sub_val)[0]\n",
    "    \n",
    "    # Get unique sessions for this subject\n",
    "    unique_sess = np.unique(ses[indices_sub])\n",
    "    \n",
    "    # Iterate over each session with a progress bar\n",
    "    for sess_val in tqdm(unique_sess, desc=f\"Processing sessions for sub {sub_val}\"):\n",
    "        # Get indices for current session\n",
    "        indices_sess = indices_sub[ses[indices_sub] == sess_val]\n",
    "        \n",
    "        # Get unique runs within this session\n",
    "        unique_runs = np.unique(run[indices_sess])\n",
    "        \n",
    "        # Iterate over each run\n",
    "        for run_val in unique_runs:\n",
    "            # Get indices for current run\n",
    "            indices_run = indices_sess[run[indices_sess] == run_val]\n",
    "            \n",
    "            # Find events where start == 0\n",
    "            indices_start0 = indices_run[start[indices_run] == 0]\n",
    "            \n",
    "            if len(indices_start0) == 0:\n",
    "                # No events found for this run\n",
    "                print(f\"  Run {run_val}: No events found (start == 0). Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Assuming events are consistent within a run, take the first occurrence\n",
    "            events_list = events[indices_start0[0]]\n",
    "            \n",
    "            # Extract timepoints and nsd_ids from events\n",
    "            timepoints = [event['index'] for event in events_list]\n",
    "            nsd_ids = [event['nsd_id'] - 1 for event in events_list]  # Adjusting nsd_id as per original code\n",
    "            \n",
    "            # Append nsd_ids to the main list\n",
    "            image_NSD73K_indices.extend(nsd_ids)\n",
    "            \n",
    "            # Iterate over each timepoint to extract sliding windows\n",
    "            for time in timepoints:\n",
    "                sliding_windows = []\n",
    "                \n",
    "                for i in range(num_TRs_per_image):\n",
    "                    # Calculate the adjusted time with delay\n",
    "                    time_ = time + i + TR_delay\n",
    "                    \n",
    "                    # Find the index where start == time_\n",
    "                    indices_time = indices_run[start[indices_run] == time_]\n",
    "                    \n",
    "                    if len(indices_time) == 0:\n",
    "                        # Handle missing data: Skip this sliding window\n",
    "                        print(f\"    Time {time_}: No feature found. Skipping this sliding window.\")\n",
    "                        break  # Exit the inner loop if any TR is missing\n",
    "                    else:\n",
    "                        # Access feature by index without loading the entire dataset\n",
    "                        sliding_window = features[indices_time[0]]\n",
    "                        sliding_windows.append(sliding_window)\n",
    "                \n",
    "                # Only append if the required number of TRs were found\n",
    "                if len(sliding_windows) == num_TRs_per_image:\n",
    "                    # Concatenate sliding windows if more than one TR per image\n",
    "                    if num_TRs_per_image > 1:\n",
    "                        sliding_window_array = np.concatenate(sliding_windows)\n",
    "                    else:\n",
    "                        sliding_window_array = sliding_windows[0]\n",
    "                    \n",
    "                    data.append(sliding_window_array)\n",
    "                else:\n",
    "                    print(f\"    Time {time}: Incomplete sliding window. Skipping.\")\n",
    "    \n",
    "# Convert the collected data and indices into NumPy arrays\n",
    "data = np.array(data)\n",
    "image_NSD73K_indices = np.array(image_NSD73K_indices)\n",
    "\n",
    "print(\"\\nProcessing Complete.\")\n",
    "print(\"Data Shape:\", data.shape)\n",
    "print(\"Image NSD73K Indices Length:\", len(image_NSD73K_indices))\n",
    "\n",
    "# Close the HDF5 file to free up resources\n",
    "data_h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d05c11e-b4f9-4411-8ab1-d2d515696a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sessions: 40\n",
      "n_runs: 13\n",
      "n_TRs (ses=1 & run=2): 285\n",
      "n_features: 71744\n"
     ]
    }
   ],
   "source": [
    "n_sessions = len(np.unique(ses)) \n",
    "n_runs = len(np.unique(run))\n",
    "n_TRs = np.sum((ses == 1) & (run == 2))\n",
    "n_features = len(sliding_window)\n",
    "print(f\"n_sessions: {n_sessions}\")\n",
    "print(f\"n_runs: {n_runs}\")\n",
    "print(f\"n_TRs (ses=1 & run=2): {n_TRs}\")\n",
    "print(f\"n_features: {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda83a7-803f-4d30-9301-0b0a533409fb",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ddf09b0-18fa-4c02-b609-2f6083a159d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 73k NSD images\n",
    "data_path = \"/weka/proj-medarc/shared/mindeyev2_dataset\"\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:]\n",
    "images = torch.Tensor(images).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f36e000-4986-414d-b746-66523221e492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27000 training trials\n",
      "1000 test trials\n"
     ]
    }
   ],
   "source": [
    "shared1000 = np.where(np.load(\"/weka/proj-medarc/shared/mindeyev2_dataset/shared1000.npy\"))[0]\n",
    "\n",
    "train_idx = [idx for idx, n in enumerate(image_NSD73K_indices) if n not in shared1000]\n",
    "train_idx = torch.Tensor(train_idx).long()\n",
    "test_idx = [int(np.where(image_NSD73K_indices == n)[0][0]) for n in shared1000] # first instance of each unique shared1000 image\n",
    "test_idx = torch.Tensor(test_idx).long()\n",
    "\n",
    "print(f\"{len(train_idx)} training trials\")\n",
    "print(f\"{len(test_idx)} test trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ba8ffe-0c74-4571-85f3-65fbcd0a42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "images = images[image_NSD73K_indices]\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80aeb2d-6d53-431c-90ed-658dca7ecebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 71744)\n",
      "torch.Size([30000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "vox = data\n",
    "print(vox.shape)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd088c32-0dc6-4607-9f93-a392e355b2bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Using Connor's old NSD-Flat that uses GLM betas, for comparison\n",
    "# directory = '/weka/proj-medarc/shared/Betas-NSD-Flat/data'\n",
    "# train_parquet_files = []\n",
    "# test_parquet_files = []\n",
    "# for filename in os.listdir(directory):\n",
    "#     if (filename.endswith(\".parquet\")) and (\"train\" in filename):\n",
    "#         train_parquet_files.append(os.path.join(directory, filename))\n",
    "#     elif (filename.endswith(\".parquet\")) and (\"test\" in filename):\n",
    "#         test_parquet_files.append(os.path.join(directory, filename))\n",
    "\n",
    "# train_nsd_ids = []\n",
    "# train_vox = []\n",
    "# flatmask = None\n",
    "# for file in tqdm(train_parquet_files):\n",
    "#     df = pd.read_parquet(file)\n",
    "#     df_filtered = df[df['subject'] == 'subj01']\n",
    "#     if len(df_filtered)>0:\n",
    "#         if flatmask is None:\n",
    "#             flatmask = np.array(Image.open(io.BytesIO(df_filtered['activity'][0]['bytes']))) - 127\n",
    "#             flatmask[flatmask!=0] = 1\n",
    "        \n",
    "#         train_nsd_ids.extend(df_filtered['nsd_id'].values)\n",
    "#         for d in df_filtered['activity']:\n",
    "#             pixels = (np.array(Image.open(io.BytesIO(d['bytes'])))[flatmask.astype(np.bool)] / 255)\n",
    "#             train_vox.append(pixels)\n",
    "# train_nsd_ids = np.array(train_nsd_ids)\n",
    "# train_vox = np.array(train_vox)\n",
    "# print(\"==Train==\")\n",
    "# print(train_nsd_ids.shape)\n",
    "# print(train_vox.shape)\n",
    "\n",
    "# test_nsd_ids = []\n",
    "# test_vox = []\n",
    "# for file in tqdm(test_parquet_files):\n",
    "#     df = pd.read_parquet(file)\n",
    "#     df_filtered = df[df['subject'] == 'subj01']\n",
    "#     if len(df_filtered)>0:\n",
    "#         test_nsd_ids.extend(df_filtered['nsd_id'].values)\n",
    "#         for d in df_filtered['activity']:\n",
    "#             pixels = (np.array(Image.open(io.BytesIO(d['bytes'])))[flatmask.astype(np.bool)] / 255)\n",
    "#             test_vox.append(pixels)\n",
    "# test_nsd_ids = np.array(test_nsd_ids)\n",
    "# test_vox = np.array(test_vox)\n",
    "\n",
    "# print(\"==Test==\")\n",
    "# print(test_nsd_ids.shape)\n",
    "# print(test_vox.shape)\n",
    "\n",
    "# # discard same-image repeats for test set\n",
    "# unique_ids, first_indices = np.unique(test_nsd_ids, return_index=True)\n",
    "# sorted_indices = np.sort(first_indices)\n",
    "# test_nsd_ids = train_nsd_ids[sorted_indices]\n",
    "# test_vox = train_vox[sorted_indices]\n",
    "\n",
    "# # # group same-image repeats for test set\n",
    "# # unique_ids, inverse_indices = np.unique(test_nsd_ids, return_inverse=True)\n",
    "# # num_unique_ids = len(unique_ids)\n",
    "\n",
    "# # sum_vox = np.zeros((num_unique_ids, test_vox.shape[1]))\n",
    "# # counts = np.zeros(num_unique_ids)\n",
    "\n",
    "# # np.add.at(sum_vox, inverse_indices, test_vox)\n",
    "# # np.add.at(counts, inverse_indices, 1)\n",
    "\n",
    "# # vox_aggregated = sum_vox / counts[:, np.newaxis]\n",
    "\n",
    "# # test_nsd_ids = unique_ids\n",
    "# # test_vox = vox_aggregated\n",
    "# # print(\"   after grouping:\")\n",
    "# print(test_nsd_ids.shape)\n",
    "# print(test_vox.shape)\n",
    "\n",
    "# # Converting nsd_ids to actual images, and converting to torch tensors\n",
    "# test_images = images[test_nsd_ids]\n",
    "# images = images[train_nsd_ids]\n",
    "\n",
    "# train_mean = np.mean(train_vox,axis=0)\n",
    "# train_std = np.std(train_vox,axis=0)\n",
    "\n",
    "# vox = utils.zscore(train_vox,train_mean=train_mean,train_std=train_std)\n",
    "# test_vox = utils.zscore(test_vox,train_mean=train_mean,train_std=train_std)\n",
    "\n",
    "# vox = torch.Tensor(vox)\n",
    "# test_vox = torch.Tensor(test_vox)\n",
    "\n",
    "# train_image_indices = np.arange(len(vox))\n",
    "# test_image_indices = np.arange(len(test_vox))\n",
    "\n",
    "# print(\"\\n ready!\")\n",
    "# print(vox.shape, images.shape)\n",
    "# print(test_vox.shape, test_images.shape)\n",
    "\n",
    "# model_name = \"Betas_NSD_Flat_testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536879cd-8863-4a70-aff4-06cb55b0b7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs have been zscored according to training set\n"
     ]
    }
   ],
   "source": [
    "train_mean = np.mean(vox[train_idx],axis=0)\n",
    "train_std = np.std(vox[train_idx],axis=0)\n",
    "\n",
    "vox = utils.zscore(vox,train_mean=train_mean,train_std=train_std)\n",
    "print(\"inputs have been zscored according to training set\")\n",
    "\n",
    "images = torch.Tensor(images)\n",
    "vox = torch.Tensor(vox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-paulscotti/foundation_env/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float32 # change depending on your mixed_precision\n",
    "\n",
    "accelerator = Accelerator(split_batches=False)# mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 24\n",
    "else:\n",
    "    batch_size = int(os.environ[\"BATCH_SIZE\"])\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 3867275\n",
      "device: cuda\n",
      "global_batch_size 24\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "print(\"global_batch_size\", global_batch_size)\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "# set data_type to match your mixed precision (automatically set based on deepspeed config)\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: NSDflat_large_gsrFalse__visualTrue_RAWepoch99\n",
      "--data_path=/weka/proj-medarc/shared/mindeyev2_dataset                     --no-multi_subject --subj=1 --batch_size=24                     --hidden_dim=1024 --clip_scale=1.                     --no-blurry_recon --blur_scale=.5                     --no-use_prior --prior_scale=30                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=15 --no-use_image_aug                     --ckpt_interval=999 --no-ckpt_saving --no-wandb_log --new_test\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \\\n",
    "                    --no-multi_subject --subj=1 --batch_size={batch_size} \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --no-blurry_recon --blur_scale=.5 \\\n",
    "                    --no-use_prior --prior_scale=30 \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=30 --no-use_image_aug \\\n",
    "                    --ckpt_interval=999 --no-ckpt_saving --no-wandb_log --new_test\"# \\\n",
    "                    #--multisubject_ckpt=../../train_logs/multisubject_subj01_1024hid_nolow_300ep_seed0\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 40\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=40,\n",
    "    help=\"Number of training sessions to include (if multi_subject, this variable doesnt matter)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--low_mem\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to preload images to cpu to speed things up but consume more memory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=2,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    import kornia.augmentation as K\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    # Define the blurring augmentations\n",
    "    blur_augment = K.RandomGaussianBlur(kernel_size=(21, 21), sigma=(51.0, 51.0), p=1.)\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 24 num_iterations_per_epoch = 1125 num_samples_per_epoch = 27000\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    # num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "    num_samples_per_epoch = len(train_idx)\n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1942b0e-1223-40e6-b543-2f7ff2e8ebcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_voxels_list = []\n",
    "num_voxels = {}\n",
    "num_voxels_list.append(vox[0].shape[-1])\n",
    "num_voxels[f'subj01'] = vox[0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db6e7c8a-3a6c-42e0-afcf-cb4c7cbf6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(train_idx)\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(test_idx)\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=1000, shuffle=False, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-paulscotti/foundation_env/lib/python3.11/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/admin/home-paulscotti/foundation_env/lib/python3.11/site-packages/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "## USING OpenCLIP ViT-bigG ###\n",
    "sys.path.append('mindeye_utils/')\n",
    "import mindeye_utils.generative_models.sgm\n",
    "from mindeye_utils.generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "from mindeye_utils.generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "try:\n",
    "    print(clip_img_embedder)\n",
    "except:\n",
    "    # # last hidden\n",
    "    # clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    #     arch=\"ViT-bigG-14\",\n",
    "    #     version=\"laion2b_s39b_b160k\",\n",
    "    #     output_tokens=True,\n",
    "    #     only_tokens=True,\n",
    "    # )\n",
    "    # clip_img_embedder.to(device)\n",
    "    # clip_seq_dim = 256\n",
    "    # clip_emb_dim = 1664\n",
    "\n",
    "    # final\n",
    "    clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "        arch=\"ViT-bigG-14\",\n",
    "        version=\"laion2b_s39b_b160k\",\n",
    "        output_tokens=False,\n",
    "        only_tokens=False,\n",
    "    )\n",
    "    clip_img_embedder.to(device)\n",
    "    clip_seq_dim = 1\n",
    "    clip_emb_dim = 1280\n",
    "\n",
    "# ## USING OPEN AI CLIP ViT-L ###\n",
    "# import clip\n",
    "# try:\n",
    "#     print(clip_model)\n",
    "# except:\n",
    "#     clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "#     preprocess = transforms.Compose([\n",
    "#         transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "#         transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "#                              std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "#     ])\n",
    "# def clip_img_embedder(image):\n",
    "#     preproc_img = preprocess(image)\n",
    "#     return clip_model.encode_image(preproc_img)\n",
    "# clip_seq_dim = 1\n",
    "# clip_emb_dim = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "73,466,880 total\n",
      "73,466,880 trainable\n",
      "param counts:\n",
      "73,466,880 total\n",
      "73,466,880 trainable\n",
      "torch.Size([2, 1, 71744]) torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len=1): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx=0):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(self.seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,1,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "14,643,736 total\n",
      "14,643,736 trainable\n",
      "param counts:\n",
      "88,110,616 total\n",
      "88,110,616 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 1, 1280]) torch.Size([2, 1, 1280]) torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=1, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        \n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        if clip_scale>0:\n",
    "            self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "            \n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b = torch.Tensor([0.]), torch.Tensor([[0.],[0.]])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        if clip_scale>0:\n",
    "            c = self.clip_proj(backbone)\n",
    "        \n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,1,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, blur_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, blur_[0].shape, blur_[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 16875\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "88,110,616 total\n",
      "88,110,616 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "# model.backbone.requires_grad_(False)\n",
    "\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    if num_iterations_per_epoch==0:\n",
    "        num_iterations_per_epoch=1\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ea5cd-f7ba-4f15-8056-3cd2535bca97",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc7b19fa-0c75-4786-b24a-3b51e1737918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inside interactive notebook. Disabling wandb and ckpt saving...\n"
     ]
    }
   ],
   "source": [
    "if utils.is_interactive():\n",
    "    print(\"Running inside interactive notebook. Disabling wandb and ckpt saving...\")\n",
    "    wandb_log = False\n",
    "    ckpt_saving = False\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'found_mindeye'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None and not resume_from_ckpt:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, train_dl, lr_scheduler = accelerator.prepare(model, optimizer, train_dl, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSDflat_large_gsrFalse__visualTrue_RAWepoch99 starting with epoch 0 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_3867275/484667093.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=data_type):\n",
      "/weka/proj-fmri/paulscotti/fMRI-foundation-model/src/mindeye_utils/generative_models/sgm/util.py:54: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(\n",
      "/tmp/ipykernel_3867275/484667093.py:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type):\n",
      "  7%|██▊                                        | 1/15 [04:09<58:19, 249.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "{'train/loss': np.float64(2.4525055616166855), 'test/loss': np.float64(4.974858283996582), 'train/lr': 0.00015589942434296661, 'train/num_steps': 1125, 'test/num_steps': 1, 'train/fwd_pct_correct': 0.2852592671016852, 'train/bwd_pct_correct': 0.22611111763119698, 'test/test_fwd_pct_correct': 0.06599999964237213, 'test/test_bwd_pct_correct': 0.039000000804662704, 'train/loss_clip_total': 2.4525055616166855, 'train/loss_blurry_total': 0.0, 'train/loss_blurry_cont_total': 0.0, 'test/loss_clip_total': 4.974858283996582, 'train/blurry_pixcorr': 0.0, 'test/blurry_pixcorr': 0.0, 'train/recon_cossim': 0.0, 'test/recon_cossim': 0.0, 'train/recon_mse': 0.0, 'test/recon_mse': 0.0, 'train/loss_prior': 0.0, 'test/loss_prior': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "for epoch in tqdm(range(epoch,num_epochs)):\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. \n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i, behav in enumerate(train_dl):  \n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0.\n",
    "            \n",
    "            behav = behav[0]\n",
    "\n",
    "            image = images[behav.long().cpu()].to(device)\n",
    "            voxel = vox[behav.long().cpu()]\n",
    "            voxel = torch.Tensor(voxel).unsqueeze(1).to(device)\n",
    "\n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            if clip_target.ndim == 2: clip_target = clip_target.unsqueeze(1)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                voxel, perm, betas, select = utils.mixco(voxel)\n",
    "\n",
    "            voxel_ridge = model.ridge(voxel,0) #[model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            # voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge) #voxel)#voxel_ridge)\n",
    "\n",
    "            if clip_scale>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                loss += loss_prior\n",
    "\n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                loss += loss_clip\n",
    "\n",
    "            if blurry_recon:     \n",
    "                image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "\n",
    "                image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    image_enc_shuf = image_enc[perm]\n",
    "                    betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                    image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                        image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "\n",
    "                image_norm = (image - mean)/std\n",
    "                image_aug = (blur_augs(image) - mean)/std\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.2)\n",
    "                loss_blurry_cont_total += cont_loss.item()\n",
    "\n",
    "                loss += (loss_blurry + 0.1*cont_loss) * blur_scale #/.18215\n",
    "\n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "            \n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "            if train_i >= num_iterations_per_epoch-1:\n",
    "                break\n",
    "                \n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type):   \n",
    "            for test_i, behav in enumerate(test_dl):\n",
    "                loss=0.\n",
    "                behav = behav[0]\n",
    "\n",
    "                image = images[behav.long().cpu()].to(device)\n",
    "                voxel = vox[behav.long().cpu()]\n",
    "\n",
    "                # image = test_images[behav.long().cpu()].to(device)\n",
    "                # voxel = test_vox[behav.long().cpu()]\n",
    "                    \n",
    "                voxel = torch.Tensor(voxel).unsqueeze(1).to(device)\n",
    "            \n",
    "                assert len(image) == 1000 #300\n",
    "            \n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "                if clip_target.ndim == 2: clip_target = clip_target.unsqueeze(1)\n",
    "            \n",
    "                voxel_ridge = model.ridge(voxel,0)\n",
    "    \n",
    "                backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)#voxel) #voxel_ridge)\n",
    "    \n",
    "                if clip_scale>0:\n",
    "                    clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                    clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                # for some evals, only doing a subset of the samples per batch because of computational cost\n",
    "                if use_prior or blurry_recon:\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone[random_samps], image_embed=clip_target[random_samps])\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                        \n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "    \n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "    \n",
    "                if blurry_recon:\n",
    "                    image_enc_pred, _ = blurry_image_enc_\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "    \n",
    "                if clip_scale>0:\n",
    "                    # forward and backward top 1 accuracy        \n",
    "                    labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                    test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                    test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            print(\"---\")\n",
    "\n",
    "            # assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    plt.show()\n",
    "\n",
    "            print(logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"{model_name}\")\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation_env",
   "language": "python",
   "name": "foundation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
