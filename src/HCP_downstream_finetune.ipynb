{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e236f1-385a-4d93-bb39-bea3ee384d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir /weka/proj-fmri/ckadirt/fMRI-foundation-model/src/checkpoints/HCPflat_large_gsrFalse_\n",
      "Loaded config.yaml from ckpt folder /weka/proj-fmri/ckadirt/fMRI-foundation-model/src/checkpoints/HCPflat_large_gsrFalse_\n",
      "\n",
      "__CONFIG__\n",
      "base_lr = 0.001\n",
      "batch_size = 32\n",
      "ckpt_interval = 5\n",
      "ckpt_saving = True\n",
      "cls_embed = True\n",
      "contrastive_loss_weight = 1.0\n",
      "datasets_to_include = HCP\n",
      "decoder_embed_dim = 512\n",
      "grad_accumulation_steps = 1\n",
      "grad_clip = 1.0\n",
      "gsr = False\n",
      "hcp_flat_path = /weka/proj-medarc/shared/HCP-Flat\n",
      "mask_ratio = 0.75\n",
      "model_name = HCPflat_large_gsrFalse_\n",
      "no_qkv_bias = False\n",
      "norm_pix_loss = False\n",
      "nsd_flat_path = /weka/proj-medarc/shared/NSD-Flat\n",
      "num_epochs = 100\n",
      "num_frames = 16\n",
      "num_samples_per_epoch = 200000\n",
      "num_workers = 10\n",
      "patch_size = 16\n",
      "pct_masks_to_decode = 1\n",
      "plotting = True\n",
      "pred_t_dim = 8\n",
      "print_interval = 20\n",
      "probe_base_lr = 0.0003\n",
      "probe_batch_size = 8\n",
      "probe_num_epochs = 30\n",
      "probe_num_samples_per_epoch = 100000\n",
      "resume_from_ckpt = True\n",
      "seed = 42\n",
      "sep_pos_embed = True\n",
      "t_patch_size = 2\n",
      "test_num_samples_per_epoch = 50000\n",
      "test_set = False\n",
      "trunc_init = False\n",
      "use_contrastive_loss = False\n",
      "wandb_log = True\n",
      "\n",
      "\n",
      "WORLD_SIZE=1\n",
      "PID of this process = 1087257\n"
     ]
    }
   ],
   "source": [
    "# Import packages and setup gpu configuration.\n",
    "# This code block shouldnt need to be adjusted!\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import utils\n",
    "from mae_utils.flat_models import *\n",
    "import h5py\n",
    "from mae_utils import flat_models\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# following fixes a Conv3D CUDNN_NOT_SUPPORTED error\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ## MODEL TO LOAD ##\n",
    "if utils.is_interactive():\n",
    "    model_name = \"HCPflat_large_gsrFalse_\"\n",
    "else:\n",
    "    model_name = sys.argv[1]\n",
    "    \n",
    "\n",
    "# outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "outdir = os.path.abspath(f'checkpoints/{model_name}')\n",
    "\n",
    "print(\"outdir\", outdir)\n",
    "# Load previous config.yaml if available\n",
    "if os.path.exists(f\"{outdir}/config.yaml\"):\n",
    "    config = yaml.load(open(f\"{outdir}/config.yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "    print(f\"Loaded config.yaml from ckpt folder {outdir}\")\n",
    "    # create global variables from the config\n",
    "    print(\"\\n__CONFIG__\")\n",
    "    for attribute_name in config.keys():\n",
    "        print(f\"{attribute_name} = {config[attribute_name]}\")\n",
    "        globals()[attribute_name] = config[f'{attribute_name}']\n",
    "    print(\"\\n\")\n",
    "\n",
    "world_size = os.getenv('WORLD_SIZE')\n",
    "if world_size is None: \n",
    "    world_size = 1\n",
    "else:\n",
    "    world_size = int(world_size)\n",
    "print(f\"WORLD_SIZE={world_size}\")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # Following allows you to change functions in models.py or utils.py and \n",
    "    # have this notebook automatically update with your revisions\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "batch_size = probe_batch_size\n",
    "num_epochs = probe_num_epochs\n",
    "\n",
    "data_type = torch.float32 # change depending on your mixed_precision\n",
    "global_batch_size = batch_size * world_size\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "hcp_flat_path = \"/weka/proj-medarc/shared/HCP-Flat\"\n",
    "# seed = 42\n",
    "# num_frames = 16\n",
    "# gsr = False\n",
    "# num_workers = 10\n",
    "# batch_size = 128\n",
    "save_ckpt = True\n",
    "wandb_log = True\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "utils.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128b23ed-5094-4f7d-8305-e5196d14e21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_pool = True\n",
      "gsr = False\n"
     ]
    }
   ],
   "source": [
    "if os.getenv('global_pool') == \"False\":\n",
    "    global_pool = False\n",
    "else:\n",
    "    global_pool = True\n",
    "print(f\"global_pool = {global_pool}\")\n",
    "\n",
    "try:\n",
    "    gsr\n",
    "except:\n",
    "    gsr = True\n",
    "    print(\"set gsr to True\")\n",
    "print(f\"gsr = {gsr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc00235a-a9e1-4dc4-8fd3-359c1d91bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UNCOMMENT THIS TO SAVE THE HCP-FLAT IN HDF5 FORMAT\n",
    "\n",
    "\n",
    "# from torch.utils.data import default_collate\n",
    "# from mae_utils.flat import load_hcp_flat_mask\n",
    "# from mae_utils.flat import create_hcp_flat\n",
    "# from mae_utils.flat import batch_unmask\n",
    "# import mae_utils.visualize as vis\n",
    "\n",
    "\n",
    "# batch_size = 26\n",
    "# print(f\"changed batch_size to {batch_size}\")\n",
    "\n",
    "# ## Test ##\n",
    "# datasets_to_include = \"HCP\"\n",
    "# assert \"HCP\" in datasets_to_include\n",
    "# test_dataset = create_hcp_flat(root=hcp_flat_path, \n",
    "#                 clip_mode=\"event\", frames=num_frames, shuffle=False, gsr=gsr, sub_list = 'test')\n",
    "# test_dl = wds.WebLoader(\n",
    "#     test_dataset.batched(batch_size, partial=False, collation_fn=default_collate),\n",
    "#     batch_size=None,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# ## Train ##\n",
    "# assert \"HCP\" in datasets_to_include\n",
    "# train_dataset = create_hcp_flat(root=hcp_flat_path, \n",
    "#                 clip_mode=\"event\", frames=num_frames, shuffle=False, gsr=gsr, sub_list = 'train')\n",
    "# train_dl = wds.WebLoader(\n",
    "#     train_dataset.batched(batch_size, partial=False, collation_fn=default_collate),\n",
    "#     batch_size=None,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# def flatten_meta(meta_dict):\n",
    "#     \"\"\"\n",
    "#     Flatten the meta dictionary by:\n",
    "#     - Replacing single-item lists with the item itself.\n",
    "#     - Converting tensors to scalar numbers.\n",
    "#     \"\"\"\n",
    "#     flattened = {}\n",
    "#     for key, value in meta_dict.items():\n",
    "#         if isinstance(value, list):\n",
    "#             if len(value) == 1:\n",
    "#                 flattened[key] = value[0]  # Replace list with its single item\n",
    "#             else:\n",
    "#                 flattened[key] = value  # Keep as is if multiple items\n",
    "#         elif isinstance(value, torch.Tensor):\n",
    "#             # Convert tensor to scalar\n",
    "#             if value.numel() == 1:\n",
    "#                 flattened[key] = value.item()\n",
    "#             else:\n",
    "#                 flattened[key] = value.tolist()  # Convert multi-element tensor to list\n",
    "#         else:\n",
    "#             flattened[key] = value  # Keep the value as is\n",
    "#     return flattened\n",
    "\n",
    "# import h5py\n",
    "# meta_array = np.array([], dtype=object)\n",
    "# # Open an HDF5 file in write mode\n",
    "# with h5py.File('train_hcp.hdf5', 'w') as h5f:\n",
    "#     flatmaps_dset = None\n",
    "    \n",
    "#     total_samples = 0\n",
    "\n",
    "#     for i, batch in tqdm(enumerate(train_dl), total = 120000):\n",
    "#         images = batch['image'][0]\n",
    "#         meta = batch['meta']\n",
    "#         batch_size = images.shape[0]\n",
    "#         meta_serializable = meta.copy()\n",
    "        \n",
    "        \n",
    "#         # Step 2: Serialize the dictionary to a JSON string\n",
    "#         meta_str = json.dumps(flatten_meta(meta_serializable), indent=4)\n",
    "#         meta_array = np.append(meta_array, meta_str)\n",
    "#         if flatmaps_dset is None:\n",
    "#             # Initialize datasets with unlimited (None) maxshape along the first axis\n",
    "#             flatmaps_shape = (0,) + images.shape[1:]\n",
    "#             flatmaps_maxshape = (None,) + images.shape[1:]\n",
    "\n",
    "#             flatmaps_dset = h5f.create_dataset(\n",
    "#                 'flatmaps',\n",
    "#                 shape=flatmaps_shape,\n",
    "#                 maxshape=flatmaps_maxshape,\n",
    "#                 dtype=np.float16,\n",
    "#                 chunks=True  # Enable chunking for efficient resizing\n",
    "#             )\n",
    "\n",
    "#         # Resize datasets to accommodate new data\n",
    "#         flatmaps_dset.resize(total_samples + batch_size, axis=0)\n",
    "\n",
    "#         # Write data to the datasets\n",
    "#         flatmaps_dset[total_samples:total_samples + batch_size] = images.numpy().astype(np.float16)\n",
    "\n",
    "#         total_samples += batch_size\n",
    "        \n",
    "#     print(f\"Processed {total_samples} samples\")\n",
    "# np.save('metadata_test_HCP.npy', meta_array)\n",
    "\n",
    "\n",
    "# import h5py\n",
    "# meta_array = np.array([], dtype=object)\n",
    "# # Open an HDF5 file in write mode\n",
    "# with h5py.File('test_hcp.hdf5', 'w') as h5f:\n",
    "#     flatmaps_dset = None\n",
    "    \n",
    "#     total_samples = 0\n",
    "\n",
    "#     for i, batch in tqdm(enumerate(test_dl), total = 12000):\n",
    "#         images = batch['image'][0]\n",
    "#         meta = batch['meta']\n",
    "#         batch_size = images.shape[0]\n",
    "#         meta_serializable = meta.copy()\n",
    "        \n",
    "        \n",
    "#         # Step 2: Serialize the dictionary to a JSON string\n",
    "#         meta_str = json.dumps(flatten_meta(meta_serializable), indent=4)\n",
    "#         meta_array = np.append(meta_array, meta_str)\n",
    "#         if flatmaps_dset is None:\n",
    "#             # Initialize datasets with unlimited (None) maxshape along the first axis\n",
    "#             flatmaps_shape = (0,) + images.shape[1:]\n",
    "#             flatmaps_maxshape = (None,) + images.shape[1:]\n",
    "\n",
    "#             flatmaps_dset = h5f.create_dataset(\n",
    "#                 'flatmaps',\n",
    "#                 shape=flatmaps_shape,\n",
    "#                 maxshape=flatmaps_maxshape,\n",
    "#                 dtype=np.float16,\n",
    "#                 chunks=True  # Enable chunking for efficient resizing\n",
    "#             )\n",
    "\n",
    "#         # Resize datasets to accommodate new data\n",
    "#         flatmaps_dset.resize(total_samples + batch_size, axis=0)\n",
    "\n",
    "#         # Write data to the datasets\n",
    "#         flatmaps_dset[total_samples:total_samples + batch_size] = images.numpy().astype(np.float16)\n",
    "\n",
    "#         total_samples += batch_size\n",
    "        \n",
    "#     print(f\"Processed {total_samples} samples\")\n",
    "# np.save('metadata_train_HCP.npy', meta_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be5d80-64ef-4e07-a15d-054dbc8b4d8a",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f04009-13f1-473d-9808-181a46d3e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "INCLUDE_CONDS = {\n",
    "    \"fear\",\n",
    "    \"neut\",\n",
    "    \"math\",\n",
    "    \"story\",\n",
    "    \"lf\",\n",
    "    \"lh\",\n",
    "    \"rf\",\n",
    "    \"rh\",\n",
    "    \"t\",\n",
    "    \"match\",\n",
    "    \"relation\",\n",
    "    \"mental\",\n",
    "    \"rnd\",\n",
    "    \"0bk_body\",\n",
    "    \"2bk_body\",\n",
    "    \"0bk_faces\",\n",
    "    \"2bk_faces\",\n",
    "    \"0bk_places\",\n",
    "    \"2bk_places\",\n",
    "    \"0bk_tools\",\n",
    "    \"2bk_tools\",\n",
    "}\n",
    "\n",
    "# test_data = []\n",
    "\n",
    "# # Iterate over the DataLoader with a progress bar\n",
    "# for sample in tqdm(train_dl, desc=\"Processing samples\"):\n",
    "#     x = sample['image']\n",
    "#     y = sample['meta']['trial_type']\n",
    "#     key = sample['meta']['key']\n",
    "#     print(x.shape, y, key)\n",
    "#     break\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(sorted(INCLUDE_CONDS))  # Ensure consistent ordering\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e66e2c0-6a73-42d7-8414-3b8acafdb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = h5py.File('/weka/proj-fmri/ckadirt/fMRI-foundation-model/src/train_hcp.hdf5', 'r')\n",
    "flatmaps_train = f_train['flatmaps']\n",
    "\n",
    "f_test = h5py.File('/weka/proj-fmri/ckadirt/fMRI-foundation-model/src/test_hcp.hdf5', 'r')\n",
    "flatmaps_test = f_test['flatmaps']\n",
    "\n",
    "metadata_train = np.load('/weka/proj-fmri/ckadirt/fMRI-foundation-model/src/metadata_train_HCP.npy', allow_pickle=True)\n",
    "metadata_test = np.load('/weka/proj-fmri/ckadirt/fMRI-foundation-model/src/metadata_test_HCP.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b52b668f-c143-40fe-b3d4-c21c7a406d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving datasets to ram\n",
      "Datasets ready\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HCPFlatDataset(Dataset):\n",
    "    def __init__(self, flatmaps, metadata):\n",
    "        self.flatmaps = flatmaps\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.flatmaps[idx], json.loads(self.metadata[idx])\n",
    "print(\"Moving datasets to ram\")\n",
    "# Loading to cpu for faster training, this can take several minutes.  Remove this [:] if you want to move one at the time.\n",
    "train_dataset = HCPFlatDataset(flatmaps_train, metadata_train)\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_dataset = HCPFlatDataset(flatmaps_test, metadata_test)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(\"Datasets ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a535979-1fc4-4e72-9fe2-96bb8897534b",
   "metadata": {},
   "source": [
    "### Creating and loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92beee5-c65a-4714-b640-ae545dc1b120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size (144, 320) patch_size (16, 16) frames 16 t_patch_size 2\n",
      "model initialized\n"
     ]
    }
   ],
   "source": [
    "from mae_utils.flat import load_hcp_flat_mask\n",
    "from mae_utils.flat import create_hcp_flat\n",
    "from mae_utils.flat import batch_unmask\n",
    "import mae_utils.visualize as vis\n",
    "\n",
    "flat_mask = load_hcp_flat_mask(hcp_flat_path)\n",
    "\n",
    "mae_model = flat_models.mae_vit_large_fmri(\n",
    "    patch_size=patch_size,\n",
    "    decoder_embed_dim=decoder_embed_dim,\n",
    "    t_patch_size=t_patch_size,\n",
    "    pred_t_dim=pred_t_dim,\n",
    "    decoder_depth=4,\n",
    "    cls_embed=cls_embed,\n",
    "    norm_pix_loss=norm_pix_loss,\n",
    "    no_qkv_bias=no_qkv_bias,\n",
    "    sep_pos_embed=sep_pos_embed,\n",
    "    trunc_init=trunc_init,\n",
    "    pct_masks_to_decode=pct_masks_to_decode,\n",
    "    img_mask=flat_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54cc1fdc-e43d-44ae-a562-a492a59580a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_checkpoint: epoch99.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1087257/781257473.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded checkpoint epoch99.pth from /weka/proj-fmri/ckadirt/fMRI-foundation-model/src/checkpoints/HCPflat_large_gsrFalse_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_files = [f for f in os.listdir(outdir) if f.endswith('.pth')]\n",
    "\n",
    "if utils.is_interactive():\n",
    "    latest_checkpoint = \"epoch99.pth\"\n",
    "else:\n",
    "    latest_checkpoint = sys.argv[2] \n",
    "print(f\"latest_checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = os.path.join(outdir, latest_checkpoint)\n",
    "\n",
    "state = torch.load(checkpoint_path)\n",
    "mae_model.load_state_dict(state[\"model_state_dict\"], strict=False)\n",
    "mae_model.to(device)\n",
    "\n",
    "print(f\"\\nLoaded checkpoint {latest_checkpoint} from {outdir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fde712b-36a7-46e7-a39a-7679e6cbe070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input except for the batch dimension\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.linear(x)\n",
    "        return out  # Raw logits\n",
    "\n",
    "# Determine the input dimension from a single sample\n",
    "# Assuming images are of shape [1, 16, 144, 320]\n",
    "input_dim = np.prod(mae_model(torch.randn(1,1,16,144,320).to(device),global_pool=global_pool, forward_features = True).shape[1:])\n",
    "print(f\"Input dimension: {input_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2c6527-fa75-42ba-be43-3d753ed8100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, lc_model, mae_model):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.lc_model = lc_model\n",
    "        self.mae_model = mae_model\n",
    "        \n",
    "        \n",
    "    def forward(self, x, gsr):\n",
    "        x = self.mae_model(x, global_pool=global_pool, forward_features = True)\n",
    "        x = self.lc_model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8983a0b-c810-4424-8c31-32b9921ebd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "lc_model = LinearClassifier(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "model = FullModel(lc_model, mae_model)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with L2 regularization (weight_decay)\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5  # Adjust based on your needs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "num_epochs = 20  # Adjust as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98629867-6d64-45ef-823d-5f4fdb279ec7",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b42844f-0b96-4e68-ae38-3af24720a803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9f9d4444-58f9-4378-9a9a-ea56bf707eb6'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "myuuid = uuid.uuid4()\n",
    "str(myuuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63ce8f26-a1b9-4e29-badc-7b9b30d0d3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in interactive notebook. Disabling W&B and ckpt saving.\n",
      "wandb_config:\n",
      " {'model_name': 'HCPflat_large_gsrFalse__HCP_FT', 'batch_size': 8, 'learning_rate': 0.0001, 'weight_decay': 1e-05, 'num_epochs': 20, 'seed': 42}\n",
      "wandb_id: HCPflat_raw_7ee35929-85c0-47da-91ab-dac2776c444f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:HCPflat_large_gsrFalse__HCP_FT_83810) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.022 MB of 0.022 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_train_loss</td><td>█▄▁</td></tr><tr><td>epoch_val_loss</td><td>▂▁█</td></tr><tr><td>train_accuracy</td><td>▁▅█</td></tr><tr><td>val_accuracy</td><td>█▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_train_loss</td><td>1.35031</td></tr><tr><td>epoch_val_loss</td><td>3.83135</td></tr><tr><td>train_accuracy</td><td>59.09091</td></tr><tr><td>val_accuracy</td><td>25</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">HCPflat_large_gsrFalse__HCP_FT</strong> at: <a href='https://stability.wandb.io/ckadirt/fMRI-foundation-model/runs/HCPflat_large_gsrFalse__HCP_FT_83810' target=\"_blank\">https://stability.wandb.io/ckadirt/fMRI-foundation-model/runs/HCPflat_large_gsrFalse__HCP_FT_83810</a><br/> View project at: <a href='https://stability.wandb.io/ckadirt/fMRI-foundation-model' target=\"_blank\">https://stability.wandb.io/ckadirt/fMRI-foundation-model</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241023_040016-HCPflat_large_gsrFalse__HCP_FT_83810/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:HCPflat_large_gsrFalse__HCP_FT_83810). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/weka/proj-fmri/ckadirt/fMRI-foundation-model/src/wandb/run-20241023_041122-HCPflat_large_gsrFalse__HCP_FT_7ee35929-85c0-47da-91ab-dac2776c444f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://stability.wandb.io/ckadirt/fMRI-foundation-model/runs/HCPflat_large_gsrFalse__HCP_FT_7ee35929-85c0-47da-91ab-dac2776c444f' target=\"_blank\">HCPflat_large_gsrFalse__HCP_FT</a></strong> to <a href='https://stability.wandb.io/ckadirt/fMRI-foundation-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://stability.wandb.io/ckadirt/fMRI-foundation-model' target=\"_blank\">https://stability.wandb.io/ckadirt/fMRI-foundation-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://stability.wandb.io/ckadirt/fMRI-foundation-model/runs/HCPflat_large_gsrFalse__HCP_FT_7ee35929-85c0-47da-91ab-dac2776c444f' target=\"_blank\">https://stability.wandb.io/ckadirt/fMRI-foundation-model/runs/HCPflat_large_gsrFalse__HCP_FT_7ee35929-85c0-47da-91ab-dac2776c444f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "if utils.is_interactive():\n",
    "    print(\"Running in interactive notebook. Disabling W&B and ckpt saving.\")\n",
    "    wandb_log = True\n",
    "    save_ckpt = True\n",
    "\n",
    "if wandb_log:\n",
    "    wandb_project = 'fMRI-foundation-model'\n",
    "    wandb_config = {\n",
    "        \"model_name\": model_name+'_HCP_FT',\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\", wandb_config)\n",
    "    random_id = str(uuid.uuid4())\n",
    "    print(\"wandb_id:\", \"HCPflat_raw\" + f\"_{random_id}\")\n",
    "    wandb.init(\n",
    "        id=model_name+'_HCP_FT' + f\"_{random_id}\",\n",
    "        project=wandb_project,\n",
    "        name=model_name+'_HCP_FT',\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43535f4-0f6e-4655-b4f0-926416685358",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    step = 0\n",
    "\n",
    "    # with torch.amp.autocast(device_type='cuda'):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    for  batch in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        images = batch[0].to(device).float().unsqueeze(1) #fix this  # Shape: [batch_size, 1, 16, 144, 320]\n",
    "        labels = batch[1]['trial_type']  # List of labels\n",
    "        \n",
    "        encoded_labels = label_encoder.transform(labels)\n",
    "        encoded_labels = torch.tensor(encoded_labels, dtype=torch.long).to(device)  # Shape: [batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, gsr=gsr)  # Shape: [num_train_samples, num_classes]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, encoded_labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        correct_train += (predicted == encoded_labels).sum().item()\n",
    "        total_train += encoded_labels.size(0)\n",
    "        \n",
    "        step = step + 1\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step [{step}/{len(train_dl)}] - Training Loss: {loss.item():.4f} - Training Accuracy: {100 * correct_train / total_train:.2f}%\")\n",
    "        # thth\n",
    "\n",
    "    epoch_train_loss = running_train_loss / total_train if total_train > 0 else 0.0\n",
    "    train_accuracy = 100 * correct_train / total_train if total_train > 0 else 0.0\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            \n",
    "            images = batch[0].to(device).float().unsqueeze(1) #fix this\n",
    "            labels = batch[1]['trial_type']\n",
    "            \n",
    "            # Encode labels to integer indices\n",
    "            encoded_labels = label_encoder.transform(labels)\n",
    "            encoded_labels = torch.tensor(encoded_labels, dtype=torch.long).to(device)\n",
    "            \n",
    "        \n",
    "            # Forward pass\n",
    "            outputs = model(images, gsr=gsr)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, encoded_labels)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == encoded_labels).sum().item()\n",
    "            total_val += encoded_labels.size(0)\n",
    "\n",
    "            \n",
    "    \n",
    "    epoch_val_loss = running_val_loss / total_val if total_val > 0 else 0.0\n",
    "    val_accuracy = 100 * correct_val / total_val if total_val > 0 else 0.0\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "        f\"- Training Loss: {epoch_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}% \"\n",
    "        f\"- Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    if wandb_log:\n",
    "        wandb.log({\n",
    "            \"epoch_train_loss\": epoch_train_loss,\n",
    "            \"epoch_val_loss\": epoch_val_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "        })\n",
    "    if save_ckpt:\n",
    "        outdir = os.path.abspath(f'checkpoints/{model_name+\"HCP_FT\"}')\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        print(\"outdir\", outdir)\n",
    "        # Save model and config\n",
    "        torch.save(model.state_dict(), f\"{outdir}/model.pth\")\n",
    "        with open(f\"{outdir}/config.yaml\", 'w') as f:\n",
    "            yaml.dump(wandb_config, f)\n",
    "        print(f\"Saved model and config to {outdir}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation_env",
   "language": "python",
   "name": "foundation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
